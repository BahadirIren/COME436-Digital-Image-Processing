{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image, ImageOps \n",
    "import time, os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_location=\"D:\\CSE504\\Deep\\IngilizAnahtari\" \n",
    "my_location2=\"D:\\CSE504\\Deep\\Tornavida\" \n",
    "veri = np.array([])\n",
    "cikis = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(my_location2): \n",
    "    print(my_location+'/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(my_location):\n",
    "    img = cv2.imread(my_location + '/' + file) \n",
    "    imgn=cv2.resize(img,(224,224)) \n",
    "    veri=np.append(veri,imgn) \n",
    "    cikis=np.append(cikis,[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(my_location2):\n",
    "    img = cv2.imread(my_location2 + '/' + file) \n",
    "    imgn=cv2.resize(img,(224,224)) \n",
    "    veri=np.append(veri,imgn) \n",
    "    cikis=np.append(cikis,[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri_n = np.reshape(veri,(-1,224,224,3)) \n",
    "veri_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cikis_n=np.reshape(cikis,(-1,2))\n",
    "print(cikis_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet = keras.Sequential()\n",
    "AlexNet.add(layers.Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), str \n",
    "AlexNet.add(layers.BatchNormalization()) \n",
    "AlexNet.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "#2nd Convolutional Layer\n",
    "AlexNet.add(layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'\n",
    "AlexNet.add(layers.BatchNormalization()) \n",
    "AlexNet.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "#3rd Convolutional Layer\n",
    "AlexNet.add(layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same' \n",
    "AlexNet.add(layers.BatchNormalization())\n",
    "#4th Convolutional Layer\n",
    "AlexNet.add(layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same' \n",
    "AlexNet.add(layers.BatchNormalization())\n",
    "#5th Convolutional Layer\n",
    "AlexNet.add(layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same' \n",
    "AlexNet.add(layers.BatchNormalization()) \n",
    "AlexNet.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "#Passing it to a Fully Connected layer\n",
    "AlexNet.add(layers.Flatten())\n",
    "# 1st Fully Connected Layer \n",
    "AlexNet.add(layers.Dense(4096, input_shape=(32,32,3,)))\n",
    "AlexNet.add(layers.BatchNormalization()) \n",
    "AlexNet.add(layers.Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "AlexNet.add(layers.Dropout(0.4))\n",
    "#2nd Fully Connected Layer\n",
    "AlexNet.add(layers.Dense(4096)) \n",
    "AlexNet.add(layers.BatchNormalization()) \n",
    "AlexNet.add(layers.Activation('relu')) \n",
    "#Add Dropout \n",
    "AlexNet.add(layers.Dropout(0.4))\n",
    "#3rd Fully Connected Layer\n",
    "AlexNet.add(layers.Dense(1000)) \n",
    "AlexNet.add(layers.BatchNormalization()) \n",
    "AlexNet.add(layers.Activation('relu')) \n",
    "#Add Dropout \n",
    "AlexNet.add(layers.Dropout(0.4))\n",
    "#Output Layer\n",
    "AlexNet.add(layers.Dense(2)) \n",
    "AlexNet.add(layers.BatchNormalization()) \n",
    "AlexNet.add(layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_veri=np.array([])\n",
    "test_veri=veri_n[1:3] \n",
    "test_veri=np.append(test_veri,veri_n[20:23]) \n",
    "test_verin=np.reshape(test_veri,(-1,224,224,3))\n",
    "test_cikis=np.array([]) test_cikis=cikis_n[1:3] \n",
    "test_cikis=np.append(test_cikis,cikis_n[20:23]) \n",
    "test_cikisn=np.reshape(test_cikis,(-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet.fit(veri_n/255.0, cikis_n, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = AlexNet.evaluate(test_verin/255.0, test_cikisn, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([AlexNet, tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = probability_model.predict(veri_n/255.0)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(224,224,3) \n",
    "model.add(layers.MaxPool2D())\n",
    "model.add(layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")) \n",
    "model.add(layers.MaxPool2D())\n",
    "model.add(layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")) \n",
    "model.add(layers.MaxPool2D())\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Flatten()) model.add(layers.Dense(128,activation=\"relu\")) \n",
    "model.add(layers.Dense(2, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(veri_n/255, cikis_n, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(veri_n/255)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm=veri_n.astype('int16') plt.figure(figsize=(10,10)) for i in range(30):\n",
    "plt.subplot(4,8,i+1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.grid(False)\n",
    "plt.imshow(imm[i,:,:], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5afd753521192693e5e546d69d7980f9f31688b70dcfe3b6261fbc7f15964264"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
